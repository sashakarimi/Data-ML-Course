{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Include-2-Shared-Functions\n",
    "\n",
    "- Import packages for computation and data manipulation\n",
    "- Set up plotting if needed\n",
    "- Set how dataframes are rendered in Jupyter notebooks\n",
    "- Define functions used in the other notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python logging to monitor gensim models\n",
    "#### TURN ON AS NEEDED ####\n",
    "#import logging\n",
    "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages for computation and data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # for number crunching\n",
    "import pandas as pd # for data loading and manipulation\n",
    "import time\n",
    "import pickle\n",
    "import re\n",
    "import random\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up plotting packages, modules, and styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pyplot module from the matplotlib library\n",
    "from matplotlib import pyplot as plt\n",
    "# Use Jupyter magics to plot inline without needing to call plt.show()\n",
    "# From the documentation (https://stackoverflow.com/questions/43027980/)\n",
    "# \"With backend = 'inline', the output of plotting commands is displayed inline within frontends \n",
    "#   like the Jupyter notebook, directly below the code cell that produced it. \n",
    "#   The resulting plots will then also be stored in the notebook document.\"\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Seaborn library (by Michael Waskom)\n",
    "import seaborn as sns\n",
    "# Set the visual styles\n",
    "sns.set(context = 'notebook', \n",
    "        style = 'darkgrid',\n",
    "        palette = 'deep', \n",
    "        font = 'sans-serif', \n",
    "        font_scale = 1.3, \n",
    "        color_codes = True, \n",
    "        rc = None\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the matplotlib styles available\n",
    "#print(plt.style.available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SET the matplotlib style HERE ####\n",
    "style = 'seaborn-darkgrid'\n",
    "plt.style.use(style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### UNCOMMENT TO TEST ####\n",
    "# Test out the style settings\n",
    "#print(\"Here's what the {} style looks like...\".format(style))\n",
    "#fig, axes = plt.subplots(1, 4, figsize=(12, 4))\n",
    "#axes[0].set_xlim(0, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotnine for ggplot\n",
    "## NOTE: This will throw the following warning:\n",
    "### FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. \n",
    "### Please use the pandas.tseries module instead. from pandas.core import datetools\n",
    "### Not sure how to handle it.\n",
    "\n",
    "#from plotnine import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter notebook display settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure all columns of a dataframe are displayed\n",
    "# https://stackoverflow.com/questions/47022070/\n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that a dataframe column value (e.g., a large text field) is not truncated\n",
    "# https://stackoverflow.com/questions/25351968\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure slide scrolling\n",
    "# from hfinger at https://github.com/damianavila/RISE/issues/185\n",
    "#### NOTE: Have to restart notebook server after running it the first time ####\n",
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager()\n",
    "cm.update('livereveal', {'width': 1024, 'height': 768, 'scroll': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of each attribute and the first n values for that attribute in the data set\n",
    "#### SET n HERE ####\n",
    "display_n = 3\n",
    "\n",
    "def get_first_n_vals(dataFrame, n=display_n):\n",
    "    feature_list = list(dataFrame)\n",
    "    first_n = [list(dataFrame[attribute][0:n]) for attribute in feature_list]\n",
    "    return list(enumerate(list(zip(feature_list, first_n))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each feature, how many/what percentage of rows are missing values?\n",
    "# From https://datascience.stackexchange.com/questions/12645/\n",
    "\n",
    "def num_missing_values_per_feature(dataFrame, display='percentage'):\n",
    "    if display == 'count':\n",
    "        return dataFrame.isnull().sum(axis=0)\n",
    "    else:\n",
    "        return dataFrame.isnull().sum(axis=0)/len(dataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penalty or Cost Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PENALTY FUNCTIONS - SOME EXAMPLES\n",
    "\n",
    "# multiplier is a positive number > 0 that determines the slope\n",
    "\n",
    "# Linear Penalty Function\n",
    "def linearPenalty(x, multiplier=1): \n",
    "    return x * multiplier\n",
    "\n",
    "# Flipped/Inverse Linear Penalty Function\n",
    "def invLinearPenalty(x, multiplier=1):\n",
    "    return -x * multiplier\n",
    "\n",
    "# Linear for negative x and zero for positive x\n",
    "def leftLinearPenalty(x, multiplier=1):\n",
    "    if(x < 0): return -x * multiplier\n",
    "    else: return 0\n",
    "    \n",
    "# Linear for positive x and zero for negative x\n",
    "def rightLinearPenalty(x, multiplier=1):\n",
    "    if(x < 0): return 0\n",
    "    else: return x * multiplier\n",
    "\n",
    "# V shape penalty\n",
    "def VPenalty(x, multiplier=1):\n",
    "    if (x < 0): return -x * multiplier\n",
    "    else: return x\n",
    "    \n",
    "# Inverted V shape penalty\n",
    "def invertedVPenalty(x, multiplier=1):\n",
    "    if (x < 0): return x * multiplier\n",
    "    else: return -x * multiplier\n",
    "    \n",
    "# Positive parabola penalty\n",
    "def squaredPenalty(x, multiplier=1):\n",
    "    return (x**2) * multiplier\n",
    "\n",
    "# Inverted parabola penalty\n",
    "def invertedSquaredPenalty(x, multiplier=1):\n",
    "    return -(x**2) * multiplier\n",
    "\n",
    "# Non-linear penalty\n",
    "def nonLinearPenalty(x, multiplier=1):\n",
    "    return x + x**2 + x**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penaltyFunctions = {linearPenalty: \"Linear Penalty\", \n",
    "                    invLinearPenalty: \"Inverse Linear Penalty\",\n",
    "                    leftLinearPenalty: \"Left-Linear Penalty\",\n",
    "                    rightLinearPenalty: \"Right-Linear Penalty\",\n",
    "                    VPenalty: \"V Penalty\",\n",
    "                    invertedVPenalty: \"Inverted-V Penalty\",\n",
    "                    squaredPenalty: \"Squared Penalty\",\n",
    "                    invertedSquaredPenalty: \"Inverted Squared Penalty\",\n",
    "                    nonLinearPenalty: \"Non-Linear Penalty\"\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the penalty function for a given list of error values and a given penalty function\n",
    "def penaltyPlot(errorList, penaltyFunction):\n",
    "    # Set up the x-axis\n",
    "    num_points = 200\n",
    "    x = np.linspace(min(errorList), max(errorList), num_points)\n",
    "    fig, ax = plt.subplots(figsize=(6,4))\n",
    "    ax.set(xlabel='Predicted Value - Actual Value')\n",
    "    ax.set(ylabel='Penalty')\n",
    "    ax.axvline(x=0, color='black')\n",
    "    ax.axhline(y=0, color='black')\n",
    "    ax.set(title=penaltyFunctions[penaltyFunction])\n",
    "    ax.plot(x, list(map(penaltyFunction,x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column of ones to the first column of a dataframe\n",
    "# and turn it into a matrix\n",
    "def df_addOnes(dataFrame):\n",
    "    vals = dataFrame.values\n",
    "    #add_ones_column = zip(np.ones(len(dataFrame)), vals)\n",
    "    #feature_matrix = np.matrix([val for val in add_ones_column])\n",
    "    feature_matrix = np.c_[np.ones(len(dataFrame)), vals]\n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making it easy to calculate the total penalty over the entire dataset\n",
    "def penalty(df_features, df_output, paramater_value_list, penalty_function):\n",
    "    \n",
    "    # df_features is a dataframe of the features (no column of ones added)\n",
    "    # df_output is a dataframe of the output column (target variable)\n",
    "    # parameter_value_list is a list of w0, w1, ..., wn+1 where n is the number of features\n",
    "    #  i.e., the number of columns in df_features.\n",
    "    \n",
    "    # Cost of being wrong calculated over the entire data set\n",
    "    # Will take X and add a first column of 1s to it to enable the matrix multiplication\n",
    "    # Therefore: X is an m x n matrix and theta is a n x 1 matrix\n",
    "    \n",
    "    #### Turn the function inputs into matrices ####\n",
    "    # Get X and y into the right shapes for use in the penalty function\n",
    "    # Add a first column of ones to the feature matrix\n",
    "    # Add a column of 1s to X \n",
    "    feature_matrix = df_addOnes(df_features)\n",
    "    output_matrix = np.matrix(df_output.values)\n",
    "    parameter_matrix = np.matrix(paramater_value_list).T\n",
    "    \n",
    "    #print(feature_matrix.shape, parameter_matrix.shape, output_matrix.shape)\n",
    "    \n",
    "    # Difference between the predicted and the actual value\n",
    "    error = (feature_matrix * parameter_matrix) - output_matrix\n",
    "    #print(error.shape)\n",
    "    \n",
    "    # penaltyPerOutput is an m x 1 matrix where each element is the penalty for\n",
    "    # the input and its associated output for a particular value of W\n",
    "    \n",
    "    # Apply a penalty function to the errors from each row of the dataset\n",
    "    penaltyPerOutput = list(map(penalty_function,error))\n",
    "    \n",
    "    # totalPenalty is the sum of the penalties of each row of the dataset\n",
    "    totalPenalty = np.sum(penaltyPerOutput)\n",
    "    \n",
    "    # The penalty of getting it wrong is 1/2m of the totalPenalty (normalized penalty)\n",
    "    # m is the number of rows in df_features\n",
    "    totalPenaltyNorm = totalPenalty / (2 * len(df_features))\n",
    "    \n",
    "    return totalPenaltyNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Gradient Descent \n",
    "# **NOTE: ONLY for a squared penalty function**\n",
    "def gradientDescent(df_features, \n",
    "                    df_output, \n",
    "                    init_params_list, \n",
    "                    num_iterations=100, \n",
    "                    learning_rate=0.0001, \n",
    "                    penalty_function=squaredPenalty):\n",
    "    # df_features is a dataframe with the features\n",
    "    # df_ouptut is a dataframe of the output column\n",
    "    # init_params_list is the list of initial W values, e.g., [-1.0, 3.53]\n",
    "    # num_iterations is the number of steps taken by the algorithm as it descends the penalty surface\n",
    "    # learning_rate is the multiplier that determines step size (smaller = smaller step size)\n",
    "    # penalty_function is the penalty function applied to the machine learning problem\n",
    "    # NOTE: The formula for gradient descent we're implementing works only for the squaredPenalty function\n",
    "    \n",
    "    # Get the inputs into matrix form so we can use matrix multiplication (more efficient)\n",
    "    feature_matrix = df_addOnes(df_features)\n",
    "    m = len(feature_matrix) # number of rows of data\n",
    "    output_matrix = np.matrix(df_output.values)\n",
    "    parameter_matrix = np.matrix(init_params_list).T\n",
    "\n",
    "    # This is the initial value of the parameters in matrix form\n",
    "    w = parameter_matrix\n",
    "    \n",
    "    # Set up arrays to capture the running results\n",
    "    # Specify dtype=object because we're putting arrays into an array\n",
    "    #running_w = np.empty(num_iterations, dtype = object)\n",
    "    #running_w = np.array([[ 0.50182941],[-0.07935517]])\n",
    "    running_w = np.array(parameter_matrix)\n",
    "    # don't have to specify dtype for the other arrays because we're putting single values into the array\n",
    "    running_error = np.zeros(num_iterations) \n",
    "    running_normError = np.zeros(num_iterations)\n",
    "    running_penalty = np.zeros(num_iterations)\n",
    "    \n",
    "    # Iterate over the dataset num_iterations times and adjust the values of the parameters each time\n",
    "    for i in range(num_iterations):\n",
    "        #print(w)\n",
    "        for j in range(len(parameter_matrix)):\n",
    "            error = ((feature_matrix * w) - output_matrix).T * np.matrix(feature_matrix[:,j]).T\n",
    "            normError = (learning_rate/m) * error\n",
    "            w[j] = w[j] - normError\n",
    "            #print(w[j])\n",
    "           \n",
    "        # w, error, normError and penalty after each iteration\n",
    "        #running_w[i] = w\n",
    "        running_w = np.append(running_w, w, axis=0)\n",
    "        #print(i)\n",
    "        #print(w)\n",
    "        #print(running_w)\n",
    "        running_error[i] = np.sum((feature_matrix * w) - output_matrix.T)\n",
    "        running_normError[i] = (learning_rate/m) * running_error[i]\n",
    "        #running_penalty[i] = penalty_function(running_error[i])\n",
    "        running_penalty[i] = penalty_function(running_normError[i])\n",
    "    \n",
    "\n",
    "    # w is the value of parameters afer num_iterations\n",
    "    #print(w)\n",
    "\n",
    "    # Get the running_w into the right form\n",
    "    # From https://jasonstitt.com/python-group-iterator-list-function\n",
    "    running_w = list(zip(*[iter(running_w)] * len(parameter_matrix)))\n",
    "    \n",
    "    # error after num_iterations\n",
    "    final_error = np.sum((feature_matrix * w) - output_matrix.T)\n",
    "\n",
    "    # Penalty after num_iterations\n",
    "    final_penalty = penalty_function(final_error)\n",
    "    \n",
    "    return w, final_penalty, running_w, running_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning functions\n",
    "\n",
    "Jason Brownlee has a good [tutorial](https://machinelearningmastery.com/clean-text-machine-learning-python/) on cleaning text using plain Python or using NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Translation table for removing punctuations\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "# Identified after an initial analysis of the corpus\n",
    "#REMOVE_WORDS = ['bc', 'at', 'asking', 'we', 'hoping', 'meeting', 'understand', 'inquiry', \n",
    "#                'could', 'need', 'request', 'looking', 'v', 'u', 'etc', 'client', 'would', \n",
    "#                'you', 'like', 'speak', 'schedule', 'call', 'analyst', 'discus', 'me', 'hi', \n",
    "#                'hello', 'follow', 'up', 'set', 'question', 'thought', 'please', 'thank',\n",
    "#               ]\n",
    "\n",
    "REMOVE_WORDS = ['i', 'im', 'id', 'would', 'hi']\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def prep_doc(document):\n",
    "    '''\n",
    "    Following https://machinelearningmastery.com/clean-text-machine-learning-python/\n",
    "    \n",
    "    1. Tokenize the entire document on whitespace.\n",
    "    2. Remove punctuation.\n",
    "    3. Normalize case.\n",
    "    4. Remove stopwords.\n",
    "    5. Lemmatize\n",
    "    6. Clean up the remaining items -- non-ASCII characters, empty strings, specific words, numbers\n",
    "    '''\n",
    "    # Tokenize\n",
    "    #tokens = nltk.word_tokenize(document)\n",
    "    tokens = document.split()\n",
    "    \n",
    "    # Strip all punctuations\n",
    "    stripped = [token.translate(table) for token in tokens]\n",
    "    \n",
    "    # Normalize case\n",
    "    normalized = [strip.lower() for strip in stripped]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stopped = [norm for norm in normalized if not norm in STOP_WORDS]\n",
    "       \n",
    "    # Lemmatize\n",
    "    lemmatized = [wordnet_lemmatizer.lemmatize(stop) for stop in stopped]\n",
    "    \n",
    "    # Remove non-ASCII tokens (e.g., '\\x96')\n",
    "    asciied = [re.sub(r'[^\\x00-\\x7F]', r'', lem) for lem in lemmatized]\n",
    "    \n",
    "    # Remove empty tokens ''\n",
    "    # Empty strings have truth value FALSE; hence non-empty strings are TRUE\n",
    "    # https://stackoverflow.com/questions/9573244/most-elegant-way-to-check-if-the-string-is-empty-in-python\n",
    "    misc = [asc for asc in asciied if asc]\n",
    "    \n",
    "    # remove strings that are numerals\n",
    "    cleaned = [mis for mis in misc if mis.isdigit() == False]\n",
    "    \n",
    "    final = [clean for clean in cleaned if not clean in REMOVE_WORDS]\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TO DO ####\n",
    "# Given a field name of a text field in a dataframe, create a text corpus.\n",
    "## The text corpus cleans each piece of text using clean_doc above\n",
    "## It then combines all of the cleaned_docs into one huge list of tokenized lists where\n",
    "## each tokenized list is a sentence from a document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple text functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_RESULTS = 50\n",
    "\n",
    "# Frequency counts of words in a corpus from the top of the list to a specific point\n",
    "## or from any specific point in the list all the way to the bottom of the list\n",
    "# Calculate the frequency of occurrence of words\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "\n",
    "def word_freq(corpus_text, num_results=NUM_RESULTS):\n",
    "    # corpus_text is the output of create_text_corpus\n",
    "    # NOTE: corpus_text MUST be a list of tokenized lists of words\n",
    "    # e.g., [[word1, ..., word7], ..., [word23, ..., word99]]\n",
    "    \n",
    "    # Stitch the lists in corpus_text into one big list of words in the entire corpus\n",
    "    corpus_words_list = [item for sublist in corpus_text for item in sublist]\n",
    "    frequency = defaultdict(int)\n",
    "    for word in corpus_words_list:\n",
    "        frequency[word] += 1\n",
    "    \n",
    "    if num_results > 0:\n",
    "        return sorted(frequency.items(), key=operator.itemgetter(1), reverse=True)[0:num_results]\n",
    "    elif num_results < 0:\n",
    "        return sorted(frequency.items(), key=operator.itemgetter(1), reverse=True)[num_results:]\n",
    "    else:\n",
    "        # return the frequencies for the entire vocabulary when num_results is set to 0\n",
    "        return sorted(frequency.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return words that occur between 2 frequency values\n",
    "def word_freq_between(text_corpus, greater_than=200, less_than=500):\n",
    "    \n",
    "    # Get the entire vocabulary by frequency\n",
    "    item_freq = word_freq(text_corpus, 0)\n",
    "    \n",
    "    return [(word[0], word[1]) for word in item_freq if (list(word)[1] >= greater_than) and (list(word)[1] < less_than)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple search functions that don't require an NLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very simple search function\n",
    "## Given a document and a word in that document, find all occurrences of the word\n",
    "## Then return the word along with its n neighbors on either side of it\n",
    "\n",
    "# If the corpus is small, then then it can be turned into a single document -- i.e.,\n",
    "## a single list of words.\n",
    "\n",
    "def get_word_context(document, word, window=6):\n",
    "    # find the indices of each occurrence of the word in the document\n",
    "    indices = [i for i, x in enumerate(document) if x == word]\n",
    "    \n",
    "    context = []\n",
    "    # get the word in context\n",
    "    for index in indices:\n",
    "        context.append(document[index-window:index+window])\n",
    "               \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very simple search function to get word context across the entire corpus\n",
    "# This is for a corpus that's built out of individual files\n",
    "def get_word_context_corpus(corpus, file_list, word, window=6):\n",
    "    \n",
    "    file_info = []\n",
    "    doc_info = []\n",
    "    \n",
    "    doc_index = 0 # keeps track of the file associated with the document\n",
    "    for document in corpus:\n",
    "        doc_name = file_list[doc_index]\n",
    "        context = get_word_context(document, word, window)\n",
    "        # if the context is not an empty list, append it to doc_info\n",
    "        if context:\n",
    "            file_info.append(doc_name)\n",
    "            context_strings = []\n",
    "            for item in context:\n",
    "                context_strings.append(\" \".join(item))\n",
    "            doc_info.append(context_strings)\n",
    "        doc_index += 1\n",
    "        \n",
    "    # Put it into a dataframe for display\n",
    "    df_results = pd.DataFrame({'Call Note': file_info, 'Matching Phrases': doc_info})\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialization (save and load) functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle a list \n",
    "import pickle\n",
    "def pickle_list(file_path, list_to_pickle):\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(list_to_pickle, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pickled list\n",
    "import pickle\n",
    "def load_list(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        unpickled_list = pickle.load(f)\n",
    "    \n",
    "    return unpickled_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize the pyLDAvis prepared file to disk\n",
    "def save_LDAvis_content(LDAvis_content, file_path):\n",
    "     with open(file_path, 'wb') as f:\n",
    "        pickle.dump(LDAvis_content, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-prepared ldavis_content from disk\n",
    "def load_LDAvis_content(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        LDAvis_prepared = pickle.load(f)\n",
    "        \n",
    "    return LDAvis_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models import Word2Vec \n",
    "from gensim.models.word2vec import LineSentence # use when reading sentences from large files\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyLDAvis for visualizing topic models\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the NMF topic model\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# scikit learn model persistance -- saving and loading\n",
    "from sklearn.externals import joblib\n",
    "# joblib.dump(scikit_model, 'filename.pkl') # save the model\n",
    "# scikit_model = joblib.load('filename.pkl') # load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_N = 20\n",
    "\n",
    "def explore_topic(nlp_model, topic_number, topn=TOP_N):\n",
    "    \"\"\"\n",
    "    accept a user-supplied nlp_model and topic number and\n",
    "    print out a formatted list of the top terms\n",
    "    \"\"\"\n",
    "        \n",
    "    print(u'{:20} {}'.format(u'Term', u'Frequency'))\n",
    "\n",
    "    for term, frequency in nlp_model.show_topic(topic_number, topn=TOP_N):\n",
    "        print(u'{:20} {:.3f}'.format(term, round(frequency, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://medium.com/ml2vec/topic-modeling-is-an-unsupervised-learning-approach-to-clustering-documents-to-discover-topics-fdfbf30e27df\n",
    "\n",
    "def get_lda_topics(model, num_topics, top_n=TOP_N):\n",
    "    '''\n",
    "    Show the words that make up the topics in an LDA topic model\n",
    "    '''\n",
    "    word_dict = {};\n",
    "    for i in range(num_topics):\n",
    "        words = model.show_topic(i, topn = top_n);\n",
    "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = [i[0] for i in words];\n",
    "    \n",
    "    return pd.DataFrame(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://medium.com/ml2vec/topic-modeling-is-an-unsupervised-learning-approach-to-clustering-documents-to-discover-topics-fdfbf30e27df\n",
    "\n",
    "def get_nmf_topics(model, num_topics, n_top_words=TOP_N):\n",
    "    \n",
    "    #the word ids obtained need to be reverse-mapped to the words so we can print the topic names.\n",
    "    vectorizer = CountVectorizer(analyzer='word')\n",
    "    feat_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    word_dict = {}\n",
    "    for i in range(num_topics):\n",
    "        \n",
    "        #for each topic, obtain the largest values, and add the words they map to into the dictionary.\n",
    "        words_ids = model.components_[i].argsort()[:-n_top_words - 1:-1]\n",
    "        words = [feat_names[key] for key in words_ids]\n",
    "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = words\n",
    "    \n",
    "    return pd.DataFrame(word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search functions on already built NLP models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the results of a query\n",
    "def get_query_results(query_string, \n",
    "                      search_dict, \n",
    "                      df_corpus, \n",
    "                      df_cols_to_display, \n",
    "                      num_results=25\n",
    "                     ):\n",
    "    \n",
    "    '''\n",
    "    query_string is a string of any length\n",
    "    search_dict is a dict that contains the name of the phraser, dictionary,\n",
    "      model, and index to use for the search.\n",
    "    df_corpus is the complete dataframe of the corpus being searched  \n",
    "    df_cols_to_display are the cols of df_corpus to display in the search results dataframe\n",
    "    \n",
    "    '''\n",
    "    t0 = time.time()\n",
    "    # Process the query string into a list of tokens\n",
    "    clean_query = prep_doc(query_string)\n",
    "    \n",
    "    # Convert the list of tokens into phrases if necessary\n",
    "    if search_dict['phraser'] != '':\n",
    "        phrased_query = search_dict['phraser'][clean_query]\n",
    "    else:\n",
    "        phrased_query = clean_query\n",
    "    \n",
    "    # For everything EXCEPT Doc2Vec proceed as follows\n",
    "    if search_dict['index'] != 'doc2vec':\n",
    "        # Use the dictionary to transform the phrased_query into a bag of words vector\n",
    "        bow_query = search_dict['dictionary'].doc2bow(phrased_query)\n",
    "    \n",
    "        # Transform the bag of words vector into a vector in the topic model's space\n",
    "        model_query = search_dict['model'][bow_query]\n",
    "    \n",
    "        # Calculate the similarity of the query to each document in the corpus\n",
    "        sims = search_dict['index'][model_query]\n",
    "    \n",
    "        # Sort the similarity scores in descending order\n",
    "        sims_sorted = sorted(enumerate(sims), key=lambda item: -item[1])[0:num_results]\n",
    "    else:\n",
    "        # Create the Doc2Vec sims on the fly\n",
    "        query_vector = search_dict['model'].infer_vector(phrased_query)\n",
    "        sims_sorted = d2v_trigram_100.docvecs.most_similar(positive=[query_vector], topn=num_results)\n",
    "    \n",
    "    # Build a dataframe for displaying the search results\n",
    "    dataFrame_content = []\n",
    "    for item in sims_sorted:\n",
    "        dataFrame_content.append(df_corpus.iloc[item[0]][df_cols_to_display].values)\n",
    "        \n",
    "    df_results = pd.DataFrame.from_records(dataFrame_content, columns=df_cols_to_display)\n",
    "    \n",
    "    t1 = time.time()\n",
    "    print(\"Search results obtained in {:.3} seconds.\".format(t1-t0))\n",
    "    print(\"The query is: {}\".format(query_string))\n",
    "    print(\"Here are the top {} results:\".format(num_results))\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaccard similarity is a metric for measuring how well a model matches an observation to an \n",
    "# existing observation.\n",
    "# Slightly modified from \n",
    "#    http://dataconomy.com/2015/04/implementing-the-five-most-popular-similarity-measures-in-python/\n",
    "def jaccard_similarity(x,y):\n",
    "    # x and y are tokenized sentences\n",
    "    #print(set(x))\n",
    "    #print(set(y))\n",
    "    intersection_cardinality = len(set.intersection(*[set(x), set(y)]))\n",
    "    union_cardinality = len(set.union(*[set(x), set(y)]))\n",
    "    \n",
    "    try:\n",
    "        jac_score = intersection_cardinality/float(union_cardinality)\n",
    "    except ZeroDivisionError:\n",
    "        jac_score = 0.\n",
    " \n",
    "    return jac_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure the jaccard similarity between the search results returned for any query\n",
    "## This is a measure of the variation in the search results that are output for any single query string\n",
    "def intra_search_overlap(df_search_output, plot_title=''): \n",
    "    '''\n",
    "    Measure the jaccard similarity between the search results returned for any query.\n",
    "    This is a measure of the variation in the search results that are output for any single query string.\n",
    "    Display the intra-search result similarity as a heatmap.\n",
    "    \n",
    "    df_search_output is the result of a search which returns ONLY the 'CLIENT_QUESTION_PROCESSED' column.\n",
    "    '''\n",
    "    \n",
    "    j_scores_intra = []\n",
    "    for i in range(len(df_search_output)):\n",
    "        j_score_row = []\n",
    "        for j in range(len(df_search_output)):\n",
    "            j_score = jaccard_similarity(df_search_output.iloc[i].values[0], \n",
    "                                         df_search_output.iloc[j].values[0]\n",
    "                                        )\n",
    "            j_score_row.append(j_score)\n",
    "    \n",
    "        j_scores_intra.append(j_score_row)\n",
    "\n",
    "    # Put the jaccard scores in a dataframe for display using Seaborn\n",
    "    df_display = pd.DataFrame(j_scores_intra, columns=list(range(0,len(df_search_output))))\n",
    "    \n",
    "    # Create the heatmap\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.set_title(plot_title)\n",
    "    sns.heatmap(df_display, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inter_search_overlap(df_search_ouputs, \n",
    "                         column_name='Short Description Tokens', \n",
    "                         plot_title='Inter-Search Overlap of Tech Disciplies'):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    # Join the tokens returned by each search result into a single big list of tokens for that search query\n",
    "    search_output_tokens = []\n",
    "    for df_out in df_search_outputs:\n",
    "        ab = itertools.chain(df_out[column_name].values)\n",
    "        flat_ab = [item for sublist in list(ab) for item in sublist]\n",
    "        search_output_tokens.append(flat_ab)\n",
    "        \n",
    "    # for each pair of lists in search_output_tokens, get the jaccard distance\n",
    "    j_scores_inter = []\n",
    "    for i in range(len(search_output_tokens)):\n",
    "        j_score_row = []\n",
    "        for j in range(len(search_output_tokens)):\n",
    "            j_score = jaccard_similarity(search_output_tokens[i], search_output_tokens[j])\n",
    "            j_score_row.append(j_score)\n",
    "    \n",
    "        j_scores_inter.append(j_score_row)\n",
    "\n",
    "    # Put the jaccard scores in a dataframe for display using Seaborn\n",
    "    df_display = pd.DataFrame(j_scores_inter, columns=list(range(0,len(search_output_tokens))))\n",
    "    \n",
    "    #return df_display\n",
    "    # Create the heatmap\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    ax.set_title(plot_title)\n",
    "    sns.heatmap(df_display, cmap=\"BuPu\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the entire table of vector embedding values\n",
    "def display_vectors(w2v_KeyedVecs):\n",
    "    \n",
    "    #### NOTE: the full model is not used - only the model's KeyedVectors ####\n",
    "    # build a list of the terms, integer indices,\n",
    "    # and term counts from the given Word2Vec model vocabulary\n",
    "    ordered_vocab = [(term, voc.index, voc.count) for term, voc in w2v_KeyedVecs.vocab.items()]\n",
    "\n",
    "    # sort by the term counts, so the most common terms appear first\n",
    "    ordered_vocab = sorted(ordered_vocab, key=lambda item: -item[2])\n",
    "\n",
    "    # unzip the terms, integer indices, and counts into separate lists\n",
    "    ordered_terms, term_indices, term_counts = zip(*ordered_vocab)\n",
    "\n",
    "    # create a DataFrame with the food2vec vectors as data,\n",
    "    # and the terms as row labels\n",
    "    word_vectors = w2v_KeyedVecs.vectors[term_indices, :]\n",
    "    \n",
    "    # create a dataframe for displaying the vectors\n",
    "    df_display = pd.DataFrame(word_vectors, index=ordered_terms)\n",
    "    \n",
    "    return df_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_RESULTS = 20\n",
    "\n",
    "# Based on Patrick Harrison and Radim Rahurek\n",
    "\n",
    "def pos_related_terms(w2v_KeyedVec, token, topn=NUM_RESULTS):\n",
    "    \n",
    "    \"\"\"\n",
    "    look up the topn most similar terms to token\n",
    "    and print them as a formatted list\n",
    "    \"\"\"\n",
    "    try:\n",
    "        for word, similarity in w2v_KeyedVec.most_similar(positive=[token], topn=NUM_RESULTS):\n",
    "            print(u'{:20} {}'.format(word, round(similarity, 3)))\n",
    "    except KeyError:\n",
    "        print(\"Sorry, try a different term\")\n",
    "        \n",
    "def neg_related_terms(w2v_KeyedVec, token, topn=NUM_RESULTS):\n",
    "    \"\"\"\n",
    "    look up the topn most similar terms to token\n",
    "    and print them as a formatted list\n",
    "    \"\"\"\n",
    "    try:\n",
    "        for word, similarity in w2v_KeyedVec.most_similar(negative=[token], topn=NUM_RESULTS):\n",
    "            print(u'{:20} {}'.format(word, round(similarity, 3)))\n",
    "    except KeyError:\n",
    "        print(\"Sorry, try a different term\")\n",
    "        \n",
    "\n",
    "def word_algebra(w2v_KeyedVec, add_string, subtract_string, topn=NUM_RESULTS):\n",
    "    \"\"\"\n",
    "    combine the vectors associated with the words provided\n",
    "    in add_string and subtract_string, look up the topn most similar\n",
    "    terms to the combined vector, and print the result(s)\n",
    "    Use add_string=None or '' or subtract=None or '' to leave the fields empty\n",
    "    \"\"\"\n",
    "    # Prep the strings\n",
    "    if add_string != None:\n",
    "        add = prep_doc(add_string)\n",
    "    else:\n",
    "        add = add_string\n",
    "        \n",
    "    if subtract_string != None:\n",
    "        subtract = prep_doc(subtract_string)\n",
    "    else:\n",
    "        subtract = subtract_string\n",
    "    \n",
    "    try:\n",
    "        answers = w2v_KeyedVec.most_similar(positive=add, negative=subtract, topn=NUM_RESULTS)\n",
    "        for term, similarity in answers:\n",
    "            print(term)\n",
    "    except KeyError:\n",
    "        print(\"Sorry, one or more terms is not in the vocabulary - please try different terms.\")\n",
    "    \n",
    "        \n",
    "def odd_one_out(w2v_KeyedVec, token_string):\n",
    "    \n",
    "    token_list = prep_doc(token_string)\n",
    "    \n",
    "    try:\n",
    "        odd_one = w2v_KeyedVec.doesnt_match(token_list)\n",
    "    except ValueError:\n",
    "        odd_one = \"Sorry, one or more terms is not in the vocabulary - please try different terms.\"\n",
    "    \n",
    "    return odd_one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Scikit Learn t-SNE model\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_tsne_input(keyed_vectors, num_words=1000):\n",
    "    \n",
    "    '''\n",
    "    Take a set of KeyedVectors produced by a Word2Vec model and prep it \n",
    "    for input into Scikit Learn's TSNE model.\n",
    "    \n",
    "    num_words cuts down the complexity by selecting a subset of words from the vocabulary (the num_words most frequent)\n",
    "    '''\n",
    "    t0 = time.time()\n",
    "    df_vecs = display_vectors(keyed_vectors)\n",
    "    t1 = time.time()\n",
    "    print(\"t-SNE input dataframe created in {:.2f} secs.\".format(t1-t0))\n",
    "    \n",
    "    # df_vecs is the input to the t-SNE model\n",
    "    tsne = TSNE()\n",
    "    tsne_input = df_vecs.head(num_words)\n",
    "    t2 = time.time()\n",
    "    tsne_vectors = tsne.fit_transform(tsne_input.values)\n",
    "    t3 = time.time()\n",
    "    print(\"t-SNE vectors created in {:.2f} secs.\".format(t3-t2))\n",
    "    \n",
    "    # Convert the tsne_vectors into a dataframe\n",
    "    # These can then be used to visualize t-SNE using Bokeh\n",
    "    df_tsne_vectors = pd.DataFrame(tsne_vectors,\n",
    "                                   index=pd.Index(tsne_input.index),\n",
    "                                   columns=[u'x_coord', u'y_coord']\n",
    "                                  )\n",
    "    \n",
    "    return df_tsne_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import HoverTool, ColumnDataSource, value\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "def plot_tsne(df_tsne_vectors, dot_color='orange', title_add=''):\n",
    "    '''\n",
    "    Visualize the t-SNE vectors using Bokeh.\n",
    "    '''\n",
    "    \n",
    "    # Map the vocabulary to the t-SNE vectors\n",
    "    df_tsne_vectors[u'word'] = df_tsne_vectors.index\n",
    "    \n",
    "    # add df_tsne_vectors as a ColumnDataSource for Bokeh\n",
    "    plot_data = ColumnDataSource(df_tsne_vectors)\n",
    "    \n",
    "    # create the plot and configure the\n",
    "    ## title, dimensions, and tools\n",
    "    tsne_plot = figure(title=u't-SNE Word Embeddings' + title_add,\n",
    "                       plot_width = 800,\n",
    "                       plot_height = 800,\n",
    "                       tools= (u'pan, wheel_zoom, box_zoom,'\n",
    "                               u'box_select, reset'),\n",
    "                       active_scroll=u'wheel_zoom')\n",
    "\n",
    "    # add a hover tool to display words on roll-over\n",
    "    tsne_plot.add_tools( HoverTool(tooltips = u'@word') )\n",
    "\n",
    "    # draw the words as circles on the plot\n",
    "    tsne_plot.circle(u'x_coord', u'y_coord', source=plot_data,\n",
    "                     color=dot_color, line_alpha=0.2, fill_alpha=0.1,\n",
    "                     size=10, hover_line_color=u'black')\n",
    "\n",
    "    # configure visual elements of the plot\n",
    "    tsne_plot.title.text_font_size = value(u'16pt')\n",
    "    tsne_plot.xaxis.visible = False\n",
    "    tsne_plot.yaxis.visible = False\n",
    "    tsne_plot.grid.grid_line_color = None\n",
    "    tsne_plot.outline_line_color = None\n",
    "\n",
    "    # Display the plot\n",
    "    show(tsne_plot);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
